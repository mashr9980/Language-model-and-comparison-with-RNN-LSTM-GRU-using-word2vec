{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvHoxmawEanF"
      },
      "source": [
        "# Task 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "U5Gsemv1zWPp"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import gensim\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_wmJxhwzVqd",
        "outputId": "4d56d75f-6d4e-4dd5-d1c1-86524269eca7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-36-6b4191d545d7>:7: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
            "\n",
            "\n",
            "  df = pd.read_csv(url, sep='\\t', compression='gzip', error_bad_lines=False, dtype='str')\n",
            "Skipping line 20773: expected 15 fields, saw 22\n",
            "Skipping line 39834: expected 15 fields, saw 22\n",
            "Skipping line 52957: expected 15 fields, saw 22\n",
            "Skipping line 54540: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 80276: expected 15 fields, saw 22\n",
            "Skipping line 96168: expected 15 fields, saw 22\n",
            "Skipping line 96866: expected 15 fields, saw 22\n",
            "Skipping line 98175: expected 15 fields, saw 22\n",
            "Skipping line 112539: expected 15 fields, saw 22\n",
            "Skipping line 119377: expected 15 fields, saw 22\n",
            "Skipping line 120065: expected 15 fields, saw 22\n",
            "Skipping line 124703: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 134024: expected 15 fields, saw 22\n",
            "Skipping line 153938: expected 15 fields, saw 22\n",
            "Skipping line 156225: expected 15 fields, saw 22\n",
            "Skipping line 168603: expected 15 fields, saw 22\n",
            "Skipping line 187002: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 200397: expected 15 fields, saw 22\n",
            "Skipping line 203809: expected 15 fields, saw 22\n",
            "Skipping line 207680: expected 15 fields, saw 22\n",
            "Skipping line 223421: expected 15 fields, saw 22\n",
            "Skipping line 244032: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 270329: expected 15 fields, saw 22\n",
            "Skipping line 276484: expected 15 fields, saw 22\n",
            "Skipping line 304755: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 379449: expected 15 fields, saw 22\n",
            "Skipping line 386191: expected 15 fields, saw 22\n",
            "Skipping line 391811: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 414348: expected 15 fields, saw 22\n",
            "Skipping line 414773: expected 15 fields, saw 22\n",
            "Skipping line 417572: expected 15 fields, saw 22\n",
            "Skipping line 419496: expected 15 fields, saw 22\n",
            "Skipping line 430528: expected 15 fields, saw 22\n",
            "Skipping line 442230: expected 15 fields, saw 22\n",
            "Skipping line 450931: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 465377: expected 15 fields, saw 22\n",
            "Skipping line 467685: expected 15 fields, saw 22\n",
            "Skipping line 485055: expected 15 fields, saw 22\n",
            "Skipping line 487220: expected 15 fields, saw 22\n",
            "Skipping line 496076: expected 15 fields, saw 22\n",
            "Skipping line 512269: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 529505: expected 15 fields, saw 22\n",
            "Skipping line 531286: expected 15 fields, saw 22\n",
            "Skipping line 535424: expected 15 fields, saw 22\n",
            "Skipping line 569898: expected 15 fields, saw 22\n",
            "Skipping line 586293: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 593880: expected 15 fields, saw 22\n",
            "Skipping line 599274: expected 15 fields, saw 22\n",
            "Skipping line 607961: expected 15 fields, saw 22\n",
            "Skipping line 612413: expected 15 fields, saw 22\n",
            "Skipping line 615913: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 677580: expected 15 fields, saw 22\n",
            "Skipping line 687191: expected 15 fields, saw 22\n",
            "Skipping line 710819: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 728692: expected 15 fields, saw 22\n",
            "Skipping line 730216: expected 15 fields, saw 22\n",
            "Skipping line 758397: expected 15 fields, saw 22\n",
            "Skipping line 760061: expected 15 fields, saw 22\n",
            "Skipping line 768935: expected 15 fields, saw 22\n",
            "Skipping line 769483: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 822725: expected 15 fields, saw 22\n",
            "Skipping line 823621: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 857041: expected 15 fields, saw 22\n",
            "Skipping line 857320: expected 15 fields, saw 22\n",
            "Skipping line 858565: expected 15 fields, saw 22\n",
            "Skipping line 860629: expected 15 fields, saw 22\n",
            "Skipping line 864033: expected 15 fields, saw 22\n",
            "Skipping line 868673: expected 15 fields, saw 22\n",
            "Skipping line 869189: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 938605: expected 15 fields, saw 22\n",
            "Skipping line 940100: expected 15 fields, saw 22\n",
            "Skipping line 975137: expected 15 fields, saw 22\n",
            "Skipping line 976314: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 985597: expected 15 fields, saw 22\n",
            "Skipping line 990873: expected 15 fields, saw 22\n",
            "Skipping line 991806: expected 15 fields, saw 22\n",
            "Skipping line 1019808: expected 15 fields, saw 22\n",
            "Skipping line 1021526: expected 15 fields, saw 22\n",
            "Skipping line 1023905: expected 15 fields, saw 22\n",
            "Skipping line 1044207: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 1084683: expected 15 fields, saw 22\n",
            "Skipping line 1093288: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 1136430: expected 15 fields, saw 22\n",
            "Skipping line 1139815: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 1179821: expected 15 fields, saw 22\n",
            "Skipping line 1195351: expected 15 fields, saw 22\n",
            "Skipping line 1202007: expected 15 fields, saw 22\n",
            "Skipping line 1224868: expected 15 fields, saw 22\n",
            "Skipping line 1232490: expected 15 fields, saw 22\n",
            "Skipping line 1238697: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 1258654: expected 15 fields, saw 22\n",
            "Skipping line 1279948: expected 15 fields, saw 22\n",
            "Skipping line 1294360: expected 15 fields, saw 22\n",
            "Skipping line 1302240: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 1413654: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 1687095: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 1805966: expected 15 fields, saw 22\n",
            "\n",
            "Skipping line 1892134: expected 15 fields, saw 22\n",
            "\n"
          ]
        }
      ],
      "source": [
        "#Assining URL to variable to get the dataset.\n",
        "\n",
        "url = \"https://web.archive.org/web/20201127142707if_/https://s3.amazonaws.com/amazon-reviews-pds/tsv/amazon_reviews_us_Office_Products_v1_00.tsv.gz\"\n",
        "\n",
        "# Loading dataset in dataframe\n",
        "\n",
        "df = pd.read_csv(url, sep='\\t', compression='gzip', error_bad_lines=False, dtype='str')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Converting gzip to TSV then make a balanced dataset and save in a CSV format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset from a TSV file\n",
        "file_path = \"amazon_reviews_us_Office_Products_v1_00.tsv\"  \n",
        "df = pd.read_csv(file_path, sep='\\t', error_bad_lines=False, dtype='str')\n",
        "\n",
        "# Convert 'star_rating' column to integers after cleaning\n",
        "df['star_rating'] = df['star_rating'].str.extract('(\\d+)').astype(float).fillna(0).astype(int)\n",
        "\n",
        "# Randomly shuffle the dataset\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Define the number of positive (high-rated) and negative (low-rated) reviews to include\n",
        "sample_size = 1000000 // 2  # Balanced dataset with 50K positive and 500K negative reviews\n",
        "\n",
        "# Select positive (4 and 5-star) and negative (1 and 2-star) reviews\n",
        "positive_reviews = df[df['star_rating'] >= 4].sample(n=sample_size, random_state=42)\n",
        "negative_reviews = df[df['star_rating'] <= 2].sample(n=sample_size, random_state=42)\n",
        "\n",
        "# Concatenate positive and negative samples to create the balanced dataset\n",
        "balanced_dataset = pd.concat([positive_reviews, negative_reviews], ignore_index=True)\n",
        "\n",
        "# Shuffle the balanced dataset again (optional)\n",
        "balanced_dataset = balanced_dataset.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Save the balanced dataset to a new CSV file\n",
        "balanced_dataset.to_csv(\"balanced_reviews.csv\", index=False)\n",
        "\n",
        "# Display the first few rows of the balanced dataset\n",
        "print(balanced_dataset.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "RNxPF2J4i1ud"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('balanced_reviews.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C2waEyOElgC"
      },
      "source": [
        "# Task 2 Part a & b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JbnPxt-o4QA-",
        "outputId": "3210e1f3-c97b-44e4-852c-262ee91028d3"
      },
      "outputs": [],
      "source": [
        "# Gensim Model downloaded from the site of google.\n",
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dQrXJsiK65aU"
      },
      "outputs": [],
      "source": [
        "pretrained_model = wv\n",
        "amazon_reviews = df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3PP18G2N6tYL"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(amazon_reviews['review_body'], amazon_reviews['star_rating'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CKjH8ZjN0grx"
      },
      "outputs": [],
      "source": [
        "X_train = X_train.astype(str)\n",
        "\n",
        "sentences = [review.split() for review in X_train]\n",
        "\n",
        "# Embedding size and window size\n",
        "embedding_size = 300\n",
        "window_size = 13\n",
        "min_word_count = 9\n",
        "\n",
        "# Train Word2Vec model\n",
        "Word2vecModel = Word2Vec(sentences, vector_size=embedding_size, window=window_size, min_count=min_word_count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqBQzuW1LpIU"
      },
      "source": [
        "# 2a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G2TdR__C-Dt_",
        "outputId": "d6c64e60-a5d9-4bba-f68d-2e40a98ace99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Error: \"Key 'king' not present\"\n",
            "One or more words were not found in the Word2Vec model vocabulary.\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    similarity3 = Word2vecModel.wv.similarity('king', 'queen')\n",
        "    similarity4 = Word2vecModel.wv.similarity('excellent', 'outstanding')\n",
        "    print(f\"Semantic Similarity between 'king' and 'queen' ( Model): {similarity3}\")\n",
        "    print(f\"Semantic Similarity between 'excellent' and 'outstanding' ( Model): {similarity4}\")\n",
        "except KeyError as e:\n",
        "    print(f\"Error: {e}\")\n",
        "    print(\"One or more words were not found in the Word2Vec model vocabulary.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N62PF2oOLs0w"
      },
      "source": [
        "# 2b"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zxjBbuYK8pH",
        "outputId": "a91499e8-3dda-464b-f10b-ff91ed0066ce"
      },
      "outputs": [],
      "source": [
        "# Example 1: King - Man + Woman = Queen\n",
        "result1 = pretrained_model.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)\n",
        "print(f\"King - Man + Woman = {result1[0][0]}\")\n",
        "\n",
        "# Example 2: Excellent ~ Outstanding\n",
        "result2 = pretrained_model.most_similar(positive=['excellent', 'outstanding'], topn=1)\n",
        "print(f\"Excellent ~ Outstanding = {result2[0][0]}\")\n",
        "\n",
        "# Example 3: Paris - France + Italy = Rome\n",
        "result3 = pretrained_model.most_similar(positive=['paris', 'italy'], negative=['france'], topn=1)\n",
        "print(f\"Paris - France + Italy = {result3[0][0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du5LNGtPdnNa"
      },
      "source": [
        "#3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "hYZQBpprL_1z"
      },
      "outputs": [],
      "source": [
        "X_test = X_test.fillna('')\n",
        "tfidf_vectorizer = TfidfVectorizer(max_features=300)  # Adjust max_features as needed\n",
        "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HjTo9SoTO7Xv",
        "outputId": "062ac735-8aa7-4f73-9f8f-982c139bffa7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perceptron Accuracy (Word2Vec): 0.55\n",
            "SVM Accuracy (Word2Vec): 0.6\n"
          ]
        }
      ],
      "source": [
        "\n",
        "perceptron_w2v = Perceptron()\n",
        "perceptron_w2v.fit(X_train_tfidf.toarray(), y_train)\n",
        "y_pred_perceptron_w2v = perceptron_w2v.predict(X_test_tfidf.toarray())\n",
        "accuracy_perceptron_w2v = accuracy_score(y_test, y_pred_perceptron_w2v)\n",
        "print(f\"Perceptron Accuracy (Word2Vec): {accuracy_perceptron_w2v}\")\n",
        "\n",
        "# SVM with Word2Vec features\n",
        "svm_w2v = SVC()\n",
        "svm_w2v.fit(X_train_tfidf.toarray(), y_train)\n",
        "y_pred_svm_w2v = svm_w2v.predict(X_test_tfidf.toarray())\n",
        "accuracy_svm_w2v = accuracy_score(y_test, y_pred_svm_w2v)\n",
        "print(f\"SVM Accuracy (Word2Vec): {accuracy_svm_w2v}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxuJs-N7dqWh"
      },
      "source": [
        "#4 MLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "koBe58tCMZXC"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Dell\\AppData\\Local\\Temp\\ipykernel_16368\\1805663092.py:14: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:264.)\n",
            "  return torch.tensor(vectors, dtype=torch.float32)\n"
          ]
        }
      ],
      "source": [
        "model = Word2Vec(sentences, vector_size=embedding_size, window=5, min_count=5)\n",
        "embedding_size = 300\n",
        "sentences = [review.split() for review in X_train]\n",
        "\n",
        "def compute_avg_w2v_vectors(sentences, model, embedding_size):\n",
        "    vectors = []\n",
        "    for sentence in sentences:\n",
        "        vector = [model.wv[word] for word in sentence if word in model.wv.index_to_key]\n",
        "        if not vector:\n",
        "            # If none of the words in the sentence are in the vocabulary, use a zero vector\n",
        "            vectors.append(np.zeros(embedding_size))\n",
        "        else:\n",
        "            vectors.append(np.mean(vector, axis=0))\n",
        "    return torch.tensor(vectors, dtype=torch.float32)\n",
        "\n",
        "X_train_avg_w2v = compute_avg_w2v_vectors(sentences, model, embedding_size)\n",
        "X_test_avg_w2v = compute_avg_w2v_vectors([review.split() for review in X_test], model, embedding_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "UJ3Z8JfeOLpo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Loss: 1.5346\n",
            "Epoch [2/20], Loss: 1.5760\n",
            "Epoch [3/20], Loss: 1.4238\n",
            "Epoch [4/20], Loss: 1.4947\n",
            "Epoch [5/20], Loss: 1.4399\n",
            "Epoch [6/20], Loss: 1.4763\n",
            "Epoch [7/20], Loss: 1.4334\n",
            "Epoch [8/20], Loss: 1.5213\n",
            "Epoch [9/20], Loss: 1.3632\n",
            "Epoch [10/20], Loss: 1.4059\n",
            "Epoch [11/20], Loss: 1.5216\n",
            "Epoch [12/20], Loss: 1.4947\n",
            "Epoch [13/20], Loss: 1.4611\n",
            "Epoch [14/20], Loss: 1.4268\n",
            "Epoch [15/20], Loss: 1.4320\n",
            "Epoch [16/20], Loss: 1.4799\n",
            "Epoch [17/20], Loss: 1.4367\n",
            "Epoch [18/20], Loss: 1.3812\n",
            "Epoch [19/20], Loss: 1.3838\n",
            "Epoch [20/20], Loss: 1.4262\n",
            "Accuracy on Testing Split: 40.00%\n",
            "Decoded Predictions: [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
          ]
        }
      ],
      "source": [
        "# Check if a GPU is available, and if not, use the CPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Define the MLP model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1, hidden_size2, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(hidden_size2, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu1(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu2(out)\n",
        "        out = self.fc3(out)\n",
        "        return out\n",
        "\n",
        "# Define hyperparameters\n",
        "input_size = embedding_size\n",
        "hidden_size1 = 50\n",
        "hidden_size2 = 5\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Move the model to the GPU\n",
        "mlp_model = MLP(input_size, hidden_size1, hidden_size2, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
        "batch_size = 64\n",
        "num_epochs = 20\n",
        "\n",
        "# Move the data to the GPU\n",
        "X_train_avg_w2v = X_train_avg_w2v.to(device)\n",
        "y_train_encoded = torch.tensor(y_train_encoded, dtype=torch.int64).to(device)\n",
        "X_test_avg_w2v = X_test_avg_w2v.to(device)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_avg_w2v, y_train_encoded)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = mlp_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluate the MLP model on the testing split\n",
        "with torch.no_grad():\n",
        "    mlp_model.eval()\n",
        "    outputs = mlp_model(X_test_avg_w2v)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    accuracy = (predicted == torch.tensor(y_test_encoded, dtype=torch.int64).to(device)).sum().item() / len(y_test_encoded)\n",
        "    print(f'Accuracy on Testing Split: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Decode labels\n",
        "decoded_predictions = label_encoder.inverse_transform(predicted.cpu().numpy())\n",
        "print(\"Decoded Predictions:\", decoded_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OR3nO_bXq12G"
      },
      "source": [
        "# 5a RNN model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Ojpji9bkn-tq"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/20], Loss: 1.2742\n",
            "Epoch [2/20], Loss: 1.3480\n",
            "Epoch [3/20], Loss: 1.3688\n",
            "Epoch [4/20], Loss: 1.3209\n",
            "Epoch [5/20], Loss: 1.3401\n",
            "Epoch [6/20], Loss: 1.3370\n",
            "Epoch [7/20], Loss: 1.3610\n",
            "Epoch [8/20], Loss: 1.3203\n",
            "Epoch [9/20], Loss: 1.3025\n",
            "Epoch [10/20], Loss: 1.2617\n",
            "Epoch [11/20], Loss: 1.2866\n",
            "Epoch [12/20], Loss: 1.3816\n",
            "Epoch [13/20], Loss: 1.2688\n",
            "Epoch [14/20], Loss: 1.2414\n",
            "Epoch [15/20], Loss: 1.2812\n",
            "Epoch [16/20], Loss: 1.2844\n",
            "Epoch [17/20], Loss: 1.2904\n",
            "Epoch [18/20], Loss: 1.2775\n",
            "Epoch [19/20], Loss: 1.2941\n",
            "Epoch [20/20], Loss: 1.1929\n",
            "Accuracy on Testing Split (Simple RNN): 40.00%\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Define the Simple RNN model\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Define hyperparameters\n",
        "input_size = embedding_size\n",
        "hidden_size = 10\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Move the model to the GPU\n",
        "simple_rnn_model = SimpleRNN(input_size, hidden_size, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(simple_rnn_model.parameters(), lr=0.001)\n",
        "batch_size = 64\n",
        "num_epochs = 20\n",
        "\n",
        "# Move the data to the GPU\n",
        "X_train_avg_w2v = X_train_avg_w2v.to(device)\n",
        "y_train_encoded = torch.tensor(y_train_encoded, dtype=torch.int64).to(device)\n",
        "X_test_avg_w2v = X_test_avg_w2v.to(device)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_avg_w2v, y_train_encoded)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = simple_rnn_model(inputs.unsqueeze(1))\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluate the Simple RNN model on the testing split\n",
        "with torch.no_grad():\n",
        "    simple_rnn_model.eval()\n",
        "    outputs = simple_rnn_model(X_test_avg_w2v.unsqueeze(1))\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    accuracy = (predicted == torch.tensor(y_test_encoded, dtype=torch.int64).to(device)).sum().item() / len(y_test_encoded)\n",
        "    print(f'Accuracy on Testing Split (Simple RNN): {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RA2UynJq4bD"
      },
      "source": [
        "#5b GRU model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "-ByJiTzRofbj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/50], Loss: 1.3505\n",
            "Epoch [2/50], Loss: 1.3184\n",
            "Epoch [3/50], Loss: 1.2855\n",
            "Epoch [4/50], Loss: 1.3717\n",
            "Epoch [5/50], Loss: 1.4305\n",
            "Epoch [6/50], Loss: 1.3379\n",
            "Epoch [7/50], Loss: 1.3707\n",
            "Epoch [8/50], Loss: 1.3899\n",
            "Epoch [9/50], Loss: 1.3965\n",
            "Epoch [10/50], Loss: 1.3713\n",
            "Epoch [11/50], Loss: 1.3546\n",
            "Epoch [12/50], Loss: 1.4280\n",
            "Epoch [13/50], Loss: 1.3559\n",
            "Epoch [14/50], Loss: 1.3258\n",
            "Epoch [15/50], Loss: 1.2483\n",
            "Epoch [16/50], Loss: 1.2677\n",
            "Epoch [17/50], Loss: 1.3434\n",
            "Epoch [18/50], Loss: 1.3376\n",
            "Epoch [19/50], Loss: 1.3580\n",
            "Epoch [20/50], Loss: 1.3703\n",
            "Epoch [21/50], Loss: 1.3562\n",
            "Epoch [22/50], Loss: 1.3206\n",
            "Epoch [23/50], Loss: 1.2983\n",
            "Epoch [24/50], Loss: 1.2842\n",
            "Epoch [25/50], Loss: 1.3310\n",
            "Epoch [26/50], Loss: 1.2908\n",
            "Epoch [27/50], Loss: 1.2744\n",
            "Epoch [28/50], Loss: 1.3169\n",
            "Epoch [29/50], Loss: 1.3577\n",
            "Epoch [30/50], Loss: 1.3374\n",
            "Epoch [31/50], Loss: 1.3163\n",
            "Epoch [32/50], Loss: 1.2702\n",
            "Epoch [33/50], Loss: 1.2534\n",
            "Epoch [34/50], Loss: 1.3114\n",
            "Epoch [35/50], Loss: 1.3092\n",
            "Epoch [36/50], Loss: 1.3440\n",
            "Epoch [37/50], Loss: 1.2806\n",
            "Epoch [38/50], Loss: 1.2366\n",
            "Epoch [39/50], Loss: 1.2858\n",
            "Epoch [40/50], Loss: 1.3765\n",
            "Epoch [41/50], Loss: 1.2218\n",
            "Epoch [42/50], Loss: 1.2936\n",
            "Epoch [43/50], Loss: 1.3226\n",
            "Epoch [44/50], Loss: 1.3483\n",
            "Epoch [45/50], Loss: 1.2634\n",
            "Epoch [46/50], Loss: 1.2133\n",
            "Epoch [47/50], Loss: 1.1926\n",
            "Epoch [48/50], Loss: 1.2930\n",
            "Epoch [49/50], Loss: 1.2882\n",
            "Epoch [50/50], Loss: 1.3044\n",
            "Accuracy on Testing Split (GRU RNN): 40.00%\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Define the GRU RNN model\n",
        "class GRURNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(GRURNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.gru(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Define hyperparameters\n",
        "input_size = embedding_size\n",
        "hidden_size = 10\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Move the model to the GPU\n",
        "gru_rnn_model = GRURNN(input_size, hidden_size, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(gru_rnn_model.parameters(), lr=0.001)\n",
        "batch_size = 64\n",
        "num_epochs = 50\n",
        "\n",
        "# Move the data to the GPU\n",
        "X_train_avg_w2v = X_train_avg_w2v.to(device)\n",
        "y_train_encoded = torch.tensor(y_train_encoded, dtype=torch.int64).to(device)\n",
        "X_test_avg_w2v = X_test_avg_w2v.to(device)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_avg_w2v, y_train_encoded)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = gru_rnn_model(inputs.unsqueeze(1))\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluate the GRU RNN model on the testing split\n",
        "with torch.no_grad():\n",
        "    gru_rnn_model.eval()\n",
        "    outputs = gru_rnn_model(X_test_avg_w2v.unsqueeze(1))\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    accuracy = (predicted == torch.tensor(y_test_encoded, dtype=torch.int64).to(device)).sum().item() / len(y_test_encoded)\n",
        "    print(f'Accuracy on Testing Split (GRU RNN): {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gH8erdSq7Av"
      },
      "source": [
        "#5c LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "MSTezNd1ozOc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/200], Loss: 1.3657\n",
            "Epoch [2/200], Loss: 1.3500\n",
            "Epoch [3/200], Loss: 1.3614\n",
            "Epoch [4/200], Loss: 1.3499\n",
            "Epoch [5/200], Loss: 1.3669\n",
            "Epoch [6/200], Loss: 1.3888\n",
            "Epoch [7/200], Loss: 1.3627\n",
            "Epoch [8/200], Loss: 1.3644\n",
            "Epoch [9/200], Loss: 1.3411\n",
            "Epoch [10/200], Loss: 1.3592\n",
            "Epoch [11/200], Loss: 1.3560\n",
            "Epoch [12/200], Loss: 1.3375\n",
            "Epoch [13/200], Loss: 1.3439\n",
            "Epoch [14/200], Loss: 1.3222\n",
            "Epoch [15/200], Loss: 1.3280\n",
            "Epoch [16/200], Loss: 1.3371\n",
            "Epoch [17/200], Loss: 1.3406\n",
            "Epoch [18/200], Loss: 1.3278\n",
            "Epoch [19/200], Loss: 1.3240\n",
            "Epoch [20/200], Loss: 1.3145\n",
            "Epoch [21/200], Loss: 1.3047\n",
            "Epoch [22/200], Loss: 1.3392\n",
            "Epoch [23/200], Loss: 1.3261\n",
            "Epoch [24/200], Loss: 1.2959\n",
            "Epoch [25/200], Loss: 1.3337\n",
            "Epoch [26/200], Loss: 1.3100\n",
            "Epoch [27/200], Loss: 1.2906\n",
            "Epoch [28/200], Loss: 1.2866\n",
            "Epoch [29/200], Loss: 1.2988\n",
            "Epoch [30/200], Loss: 1.2779\n",
            "Epoch [31/200], Loss: 1.2715\n",
            "Epoch [32/200], Loss: 1.3191\n",
            "Epoch [33/200], Loss: 1.2709\n",
            "Epoch [34/200], Loss: 1.2937\n",
            "Epoch [35/200], Loss: 1.2947\n",
            "Epoch [36/200], Loss: 1.3018\n",
            "Epoch [37/200], Loss: 1.2687\n",
            "Epoch [38/200], Loss: 1.2927\n",
            "Epoch [39/200], Loss: 1.2421\n",
            "Epoch [40/200], Loss: 1.3205\n",
            "Epoch [41/200], Loss: 1.3285\n",
            "Epoch [42/200], Loss: 1.3277\n",
            "Epoch [43/200], Loss: 1.2891\n",
            "Epoch [44/200], Loss: 1.2784\n",
            "Epoch [45/200], Loss: 1.2932\n",
            "Epoch [46/200], Loss: 1.2933\n",
            "Epoch [47/200], Loss: 1.2922\n",
            "Epoch [48/200], Loss: 1.2175\n",
            "Epoch [49/200], Loss: 1.3025\n",
            "Epoch [50/200], Loss: 1.2627\n",
            "Epoch [51/200], Loss: 1.3139\n",
            "Epoch [52/200], Loss: 1.2619\n",
            "Epoch [53/200], Loss: 1.2697\n",
            "Epoch [54/200], Loss: 1.2528\n",
            "Epoch [55/200], Loss: 1.3263\n",
            "Epoch [56/200], Loss: 1.3211\n",
            "Epoch [57/200], Loss: 1.2340\n",
            "Epoch [58/200], Loss: 1.3106\n",
            "Epoch [59/200], Loss: 1.2963\n",
            "Epoch [60/200], Loss: 1.2494\n",
            "Epoch [61/200], Loss: 1.3844\n",
            "Epoch [62/200], Loss: 1.2773\n",
            "Epoch [63/200], Loss: 1.3240\n",
            "Epoch [64/200], Loss: 1.1658\n",
            "Epoch [65/200], Loss: 1.1878\n",
            "Epoch [66/200], Loss: 1.2118\n",
            "Epoch [67/200], Loss: 1.3011\n",
            "Epoch [68/200], Loss: 1.2439\n",
            "Epoch [69/200], Loss: 1.2838\n",
            "Epoch [70/200], Loss: 1.2604\n",
            "Epoch [71/200], Loss: 1.2105\n",
            "Epoch [72/200], Loss: 1.2743\n",
            "Epoch [73/200], Loss: 1.1968\n",
            "Epoch [74/200], Loss: 1.2933\n",
            "Epoch [75/200], Loss: 1.2876\n",
            "Epoch [76/200], Loss: 1.2660\n",
            "Epoch [77/200], Loss: 1.2246\n",
            "Epoch [78/200], Loss: 1.2673\n",
            "Epoch [79/200], Loss: 1.1863\n",
            "Epoch [80/200], Loss: 1.1829\n",
            "Epoch [81/200], Loss: 1.2897\n",
            "Epoch [82/200], Loss: 1.2559\n",
            "Epoch [83/200], Loss: 1.1508\n",
            "Epoch [84/200], Loss: 1.1952\n",
            "Epoch [85/200], Loss: 1.1759\n",
            "Epoch [86/200], Loss: 1.2602\n",
            "Epoch [87/200], Loss: 1.0715\n",
            "Epoch [88/200], Loss: 1.1663\n",
            "Epoch [89/200], Loss: 1.4411\n",
            "Epoch [90/200], Loss: 1.2591\n",
            "Epoch [91/200], Loss: 1.2475\n",
            "Epoch [92/200], Loss: 1.1963\n",
            "Epoch [93/200], Loss: 1.1588\n",
            "Epoch [94/200], Loss: 1.1301\n",
            "Epoch [95/200], Loss: 1.1330\n",
            "Epoch [96/200], Loss: 1.2187\n",
            "Epoch [97/200], Loss: 1.2749\n",
            "Epoch [98/200], Loss: 1.0245\n",
            "Epoch [99/200], Loss: 1.2519\n",
            "Epoch [100/200], Loss: 1.2606\n",
            "Epoch [101/200], Loss: 1.1725\n",
            "Epoch [102/200], Loss: 1.2408\n",
            "Epoch [103/200], Loss: 1.2333\n",
            "Epoch [104/200], Loss: 1.3222\n",
            "Epoch [105/200], Loss: 1.1992\n",
            "Epoch [106/200], Loss: 1.1799\n",
            "Epoch [107/200], Loss: 1.1381\n",
            "Epoch [108/200], Loss: 1.2040\n",
            "Epoch [109/200], Loss: 1.1283\n",
            "Epoch [110/200], Loss: 1.3187\n",
            "Epoch [111/200], Loss: 1.1137\n",
            "Epoch [112/200], Loss: 1.3587\n",
            "Epoch [113/200], Loss: 1.2368\n",
            "Epoch [114/200], Loss: 1.0496\n",
            "Epoch [115/200], Loss: 1.1845\n",
            "Epoch [116/200], Loss: 1.2412\n",
            "Epoch [117/200], Loss: 1.1674\n",
            "Epoch [118/200], Loss: 1.0988\n",
            "Epoch [119/200], Loss: 1.3517\n",
            "Epoch [120/200], Loss: 1.1223\n",
            "Epoch [121/200], Loss: 1.0388\n",
            "Epoch [122/200], Loss: 1.1798\n",
            "Epoch [123/200], Loss: 1.1445\n",
            "Epoch [124/200], Loss: 1.1555\n",
            "Epoch [125/200], Loss: 1.3079\n",
            "Epoch [126/200], Loss: 1.1492\n",
            "Epoch [127/200], Loss: 1.1645\n",
            "Epoch [128/200], Loss: 1.3006\n",
            "Epoch [129/200], Loss: 1.0689\n",
            "Epoch [130/200], Loss: 1.2477\n",
            "Epoch [131/200], Loss: 1.3903\n",
            "Epoch [132/200], Loss: 1.0883\n",
            "Epoch [133/200], Loss: 1.0964\n",
            "Epoch [134/200], Loss: 1.0554\n",
            "Epoch [135/200], Loss: 1.2417\n",
            "Epoch [136/200], Loss: 1.2713\n",
            "Epoch [137/200], Loss: 1.1466\n",
            "Epoch [138/200], Loss: 1.3053\n",
            "Epoch [139/200], Loss: 1.1748\n",
            "Epoch [140/200], Loss: 1.2643\n",
            "Epoch [141/200], Loss: 1.3746\n",
            "Epoch [142/200], Loss: 1.2302\n",
            "Epoch [143/200], Loss: 1.3473\n",
            "Epoch [144/200], Loss: 1.0877\n",
            "Epoch [145/200], Loss: 1.2011\n",
            "Epoch [146/200], Loss: 1.1406\n",
            "Epoch [147/200], Loss: 1.0794\n",
            "Epoch [148/200], Loss: 1.3011\n",
            "Epoch [149/200], Loss: 1.0762\n",
            "Epoch [150/200], Loss: 1.3267\n",
            "Epoch [151/200], Loss: 1.2306\n",
            "Epoch [152/200], Loss: 1.1736\n",
            "Epoch [153/200], Loss: 1.0837\n",
            "Epoch [154/200], Loss: 1.0122\n",
            "Epoch [155/200], Loss: 1.1893\n",
            "Epoch [156/200], Loss: 1.1669\n",
            "Epoch [157/200], Loss: 1.0145\n",
            "Epoch [158/200], Loss: 1.1720\n",
            "Epoch [159/200], Loss: 1.2474\n",
            "Epoch [160/200], Loss: 1.2791\n",
            "Epoch [161/200], Loss: 1.0733\n",
            "Epoch [162/200], Loss: 1.0259\n",
            "Epoch [163/200], Loss: 1.2764\n",
            "Epoch [164/200], Loss: 1.1065\n",
            "Epoch [165/200], Loss: 1.1398\n",
            "Epoch [166/200], Loss: 1.1457\n",
            "Epoch [167/200], Loss: 1.2057\n",
            "Epoch [168/200], Loss: 1.3298\n",
            "Epoch [169/200], Loss: 0.9979\n",
            "Epoch [170/200], Loss: 1.0684\n",
            "Epoch [171/200], Loss: 1.2427\n",
            "Epoch [172/200], Loss: 1.2424\n",
            "Epoch [173/200], Loss: 1.1695\n",
            "Epoch [174/200], Loss: 1.1780\n",
            "Epoch [175/200], Loss: 1.1952\n",
            "Epoch [176/200], Loss: 1.0662\n",
            "Epoch [177/200], Loss: 1.4728\n",
            "Epoch [178/200], Loss: 1.1359\n",
            "Epoch [179/200], Loss: 1.1299\n",
            "Epoch [180/200], Loss: 1.1043\n",
            "Epoch [181/200], Loss: 1.0604\n",
            "Epoch [182/200], Loss: 1.1682\n",
            "Epoch [183/200], Loss: 1.1677\n",
            "Epoch [184/200], Loss: 1.1431\n",
            "Epoch [185/200], Loss: 1.0683\n",
            "Epoch [186/200], Loss: 1.4093\n",
            "Epoch [187/200], Loss: 1.1293\n",
            "Epoch [188/200], Loss: 1.1745\n",
            "Epoch [189/200], Loss: 1.0623\n",
            "Epoch [190/200], Loss: 1.1682\n",
            "Epoch [191/200], Loss: 1.4512\n",
            "Epoch [192/200], Loss: 1.0834\n",
            "Epoch [193/200], Loss: 1.1555\n",
            "Epoch [194/200], Loss: 1.1345\n",
            "Epoch [195/200], Loss: 1.1662\n",
            "Epoch [196/200], Loss: 1.1729\n",
            "Epoch [197/200], Loss: 1.2372\n",
            "Epoch [198/200], Loss: 1.1917\n",
            "Epoch [199/200], Loss: 1.1106\n",
            "Epoch [200/200], Loss: 1.1905\n",
            "Accuracy on Testing Split (LSTM RNN): 40.00%\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_train_encoded = label_encoder.fit_transform(y_train)\n",
        "y_test_encoded = label_encoder.transform(y_test)\n",
        "\n",
        "# Define the LSTM model\n",
        "class LSTMRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(LSTMRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.lstm(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Define hyperparameters\n",
        "input_size = embedding_size  # Input size matches the Word2Vec embedding size\n",
        "hidden_size = 10\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Move the model to the GPU\n",
        "lstm_rnn_model = LSTMRNN(input_size, hidden_size, num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(lstm_rnn_model.parameters(), lr=0.001)\n",
        "batch_size = 64\n",
        "num_epochs = 200\n",
        "\n",
        "# Move the data to the GPU\n",
        "X_train_avg_w2v = X_train_avg_w2v.to(device)\n",
        "y_train_encoded = torch.tensor(y_train_encoded, dtype=torch.int64).to(device)\n",
        "X_test_avg_w2v = X_test_avg_w2v.to(device)\n",
        "\n",
        "train_dataset = TensorDataset(X_train_avg_w2v, y_train_encoded)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (inputs, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        outputs = lstm_rnn_model(inputs.unsqueeze(1))\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
        "\n",
        "# Evaluate the LSTM RNN model on the testing split\n",
        "with torch.no_grad():\n",
        "    lstm_rnn_model.eval()\n",
        "    outputs = lstm_rnn_model(X_test_avg_w2v.unsqueeze(1))\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    accuracy = (predicted == torch.tensor(y_test_encoded, dtype=torch.int64).to(device)).sum().item() / len(y_test_encoded)\n",
        "    print(f'Accuracy on Testing Split (LSTM RNN): {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCGD1WX-pbdJ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
